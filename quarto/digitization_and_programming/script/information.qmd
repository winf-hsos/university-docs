# Informationen {#sec-information}

:::: {layout="[0.2, 0.8]" .content-visible when-format="pdf"}

:::{#firstcol}
[![](qrcodes\g-QOJ6z0VFE.png){fig-align="center" width=10%}](https://youtu.be/g-QOJ6z0VFE)
:::

:::{#secondcol}
Zu diesem Kapitel gibt es ein Video auf YouTube. Scanne einfach den QR-Code oder klicke in der digitalen Version des Buches auf den QR-Code.
:::

::::

::: {.content-visible when-format="html"}

Zu diesem Kapitel habe ich ein Video auf YouTube bereitgestellt.

{{< video https://youtu.be/g-QOJ6z0VFE >}}

:::

Im digitalen Zeitalter sind wir von **Information** umgeben - ein ständiger Strom von Nachrichten, Daten und Fakten prägt unseren Alltag. Doch was versteht die Informatik unter dem Begriff "Information"? Eine erste Annäherung bietet unsere alltägliche Erfahrung: Information entsteht, wenn wir etwas Neues erfahren, das uns hilft, bessere Entscheidungen zu treffen. Nehmen wir eine Wettervorhersage als Beispiel: Die Aussage "es wird morgen regnen" stellt für jemanden, der das Wetter noch nicht kennt, eine wichtige Information dar - sie reduziert die Unsicherheit über das morgige Wetter. Die Informatik und Informationstheorie greifen genau dieses Konzept auf und definieren **Information** präzise als die Abnahme von Unsicherheit. 

Dieses Kapitel führt systematisch in den Informationsbegriff ein und legt damit das theoretische Fundament für die nachfolgenden Themen: vom Binärsystem als Grundlage der digitalen Informationsverarbeitung über verschiedene Methoden der Informationskodierung bis hin zu praktischen Anwendungen wie Datenkompression und Verschlüsselung.

## Wie definieren wir Information?

Information begegnet uns im Alltag in vielen Formen – als Nachricht, Erkenntnis oder Mitteilung. Die Informatik verwendet jedoch einen präziseren Informationsbegriff, der sich von diesem alltäglichen Verständnis unterscheidet. Basierend auf @adami_what_2016 werden wir in diesem Kapitel eine formale Definition von Information entwickeln und anhand konkreter Beispiele veranschaulichen.

![Definition des Informationsbegriffs laut @adami_what_2016.](https://winf-hsos.github.io/university-docs/images/information_definition_adami.png){fig-align="center" width=100% .lightbox}

Um das Konzept der Information greifbar zu machen, betrachten wir ein praktisches Gedankenexperiment: ein Zahlenratespiel. Angenommen, ich denke an eine Zahl zwischen 1 und 16 (@fig-information-guess-num). Deine Aufgabe ist es, diese Zahl zu erraten. Du hast zwar nur einen finalen Rateversuch, darfst aber zuvor Fragen stellen, die ich mit "ja" oder "nein" beantworte. Diese Fragen haben die Form "Ist die Zahl größer als X?" und helfen dir, die möglichen Zahlen systematisch einzugrenzen.

Bei jeder Antwort reduziert sich deine **Unsicherheit** über die gesuchte Zahl. Durch den Ausschluss bestimmter Zahlen wird der Bereich möglicher Lösungen kleiner, und dein Wissen über die gesuchte Zahl wächst entsprechend.

**Wichtig:** Der informationstheoretische Begriff der Information unterscheidet sich vom alltäglichen Verständnis. In der Informatik geht es nicht primär um die *inhaltliche Bedeutung* (Semantik) einer Nachricht, sondern um die *quantitative Reduktion von Unsicherheit*. Die Frage ist also nicht, was eine Information inhaltlich - oder semantisch - bedeutet, sondern wie stark sie die vorhandene Ungewissheit verringert.

Dieses Prinzip der Unsicherheitsreduktion bildet das Fundament der Informationstheorie. Mit jeder Antwort, die wir erhalten, sammeln wir Information, die die Unsicherheit reduziert. Aber wie lässt sich dieser Informationsgewinn konkret messen? Welche Menge an Information steckt in einer einzelnen Antwort?

![Das Zahlenratespiel zur Veranschaulichung des Informationsbegriffs.](https://winf-hsos.github.io/university-docs/images/number_guessing_game_1.png){fig-align="center" width=100% #fig-information-guess-num .lightbox}

## Wie messen wir Information?

### Bit für Bit

Die Informatik definiert Information präzise als das, was es uns ermöglicht, richtige Vorhersagen zu treffen, die genauer sind als der pure Zufall [@adami_what_2016]. Dabei konzentriert sich diese Definition auf den messbaren Aspekt: Information reduziert systematisch die Unsicherheit über einen Sachverhalt, indem sie die Anzahl möglicher Optionen eingrenzt.

Betrachten wir nun unser Zahlenratespiel unter dem Aspekt der Informationstheorie. In diesem Beispiel reduziert jede Antwort systematisch die Anzahl der möglichen Zahlen. Diese Reduzierung der Unsicherheit lässt sich mit einer präzisen Maßeinheit quantifizieren: dem **Bit**. Ein Bit (von **binary digit**, Binärziffer) stellt die fundamentale Einheit der Information dar und entspricht einer Halbierung der Unsicherheit. Anders ausgedrückt: Wenn eine Antwort die Anzahl der möglichen Optionen exakt halbiert, gewinnen wir genau ein Bit an Information.

Allerdings werden nicht alle Fragen und deren Antworten genau ein Bit Information liefern. Wenn zum Beispiel deine erste Frage im Ratespiel lautet: "Ist deine Zahl größer als 12?" und die Antwort "nein" ist, bleiben die Zahlen 1 bis 12 übrig. Das bedeutet, du hast noch 12 Optionen von ursprünglich 16, was die Unsicherheit nicht halbiert. Es werden nur 4 statt der nötigen 8 Möglichkeiten entfernt. Lautet die Antwort dagegen “ja”, so kannst du insgesamt 12 Möglichkeiten streichen, was mehr als der Hälfte entspricht und der Informationsgehalt der Antwort wäre größer als ein Bit.

Um genau ein Bit Information zu erhalten, solltest du versuchen, mit jeder Frage genau die Hälfte der möglichen Zahlen auszuschließen. Wenn du zum Beispiel fragst "Ist deine Zahl größer als 8?", stellst du sicher, dass du - egal ob die Antwort "ja" oder "nein" ist - in beiden Fällen 8 mögliche Zahlen übrig behältst. Ist die Antwort "ja", bleiben die Zahlen 9 bis 16 übrig. Ist die Antwort "nein", bleiben die Zahlen 1 bis 8. In beiden Szenarien wird deine Unsicherheit um die Hälfte reduziert, also um ein Bit.

Indem du deine Fragen sorgfältig so wählst, dass sie die verbleibenden Optionen jedes Mal halbieren, reduzierst du deine Unsicherheit Bit für Bit und du findest schneller die richtige Zahl.

![Deine Fragen sollten die Anzahl der Möglichkeiten halbieren.](https://winf-hsos.github.io/university-docs/images/number_guessing_game_2.png){fig-align="center" width=100% .lightbox}

Angenommen, die Antwort auf deine erste Frage "Ist deine Zahl größer als 8?" war "nein". Was sollte deine nächste Frage sein, um die Unsicherheit weiterhin effektiv zu reduzieren? Die beste Strategie ist es zu fragen: "Ist sie größer als 4?". Diese Vorgehensweise stellt sicher, dass dir immer nur vier mögliche Zahlen bleiben: entweder 1 bis 4, wenn die Antwort "nein" ist, oder 5 bis 8, wenn die Antwort "ja" ist. Erneut halbiert dies die verbleibenden Optionen und liefert genau ein Bit an Information.

![Nach zwei Fragen bleiben noch 4 Möglichkeitn, wenn die Fragen gut gewählt wurden.](https://winf-hsos.github.io/university-docs/images/number_guessing_game_3.png){fig-align="center" width=100% .lightbox}

Lass uns mit dieser Methode fortfahren. Angenommen, deine zweite Frage "Ist sie größer als 4?" erhält ein "nein" als Antwort. Deine neue Auswahl an Zahlen ist auf 1, 2, 3 und 4 begrenzt. Um die Unsicherheit weiter zu reduzieren, wäre die nächste logische Frage: "Ist sie größer als 2?" Dies lässt dir entweder die Zahlen 1 und 2 oder 3 und 4, abhängig von der Antwort.

Indem du weiterhin Fragen stellst, die systematisch die verbleibenden Optionen halbieren, kannst du sehen, wie wir schrittweise die Unsicherheit Bit für Bit reduzieren. Schließlich wirst du nach nur vier Fragen die Zahl auf genau eine eingrenzen, da keine anderen Möglichkeiten mehr übrig sind. Das bedeutet, du hast die Unsicherheit auf null reduziert.

![Wir benötigen vier Ja/Nein-Fragen, um die Optionen von 16 auf eine einzige zu reduzieren.](https://winf-hsos.github.io/university-docs/images/number_guessing_game_6.png){fig-align="center" width=100% .lightbox}

### Unsicherheit

Wir haben oben gelernt, dass wir mit jeder Ja/Nein-Frage, die die Hälfte der Möglichkeiten eliminiert, ein Bit an Information gewinnen. Aber wie können wir dieses Konzept der Reduktion von Unsicherheit mathematisch quantifizieren? Wir gehen das Schritt für Schritt durch.

Stell dir vor, du bist am Anfang unseres Zahlenratespiels. Es gibt 16 mögliche Zahlen, an die ich denken könnte, daher ist deine Wahrscheinlichkeit, beim ersten Versuch die richtige Zahl zu erraten, ziemlich gering:

$$
P_{correct} = \frac{1}{16}
$$

Das entspricht einer Wahrscheinlichkeit von nur 0,0625 - definitiv nicht zu deinen Gunsten. Angenommen, du stellst eine Frage, die die Möglichkeiten auf die Hälfte reduziert. Jetzt verbessert sich deine Chance, richtig zu raten:

$$
P_{correct} = \frac{1}{8}
$$

Mit jeder weiteren Frage verdoppelt sich diese Wahrscheinlichkeit, während sich deine Unsicherheit halbiert. Ist Wahrscheinlichkeit die beste Methode, um Unsicherheit zu messen? Wenn wir möchten, dass unser Maß für Unsicherheit abnimmt, während wir mehr Informationen sammeln, ist es es sinnvoll, den Kehrwert der Wahrscheinlichkeit zu verwenden. Das entspräche der Anzahl an Möglichkeiten, die noch im Rennen sind.

Zu Beginn gibt es 16 Möglichkeiten, also begänne die Unsicherheit bei 16. Nach der ersten Frage fiele sie auf 8, dann auf 4, 2 und schließlich 1. Allerdings würde dieses Maß uns selbst dann, wenn nur noch eine Option übrig ist, eine gewisse Unsicherheit, nämlich 1, anzeigen, was nicht ganz unserem intuitiven Verständnis entspricht. Wenn wir sicher sind, dann sollte die Unsicherheit 0 sein.

Claude Shannon, der Vater der Informationstheorie, schlug einen anderen Ansatz vor. Er empfahl, die Unsicherheit mithilfe des Logarithmus zur Basis 2 der Anzahl der Möglichkeiten zu messen:

$$
H = log_2(N)
$$

Diese Methode hat mehrere Vorteile. Erstens ist die Unsicherheit gleich null, wenn es nur eine mögliche Antwort gibt (N=1), was mit unserer Intuition übereinstimmt:

$$
log_2(1) = 0
$$

Ein weiterer Vorteil ist die Einfachheit der Berechnungen, besonders wenn es um mehrere unabhängige Unsicherheiten geht. Nehmen wir an, das Spiel ändert sich: Jetzt denke ich an zwei unabhängige Zahlen zwischen 1 und 16. Bevor du irgendwelche Fragen stellst, beträgt deine Unsicherheit für jede Zahl:

$$
log_2(16)= 4~bits
$$

Die Gesamtunsicherheit für beide Zahlen ist einfach die Summe ihrer individuellen Unsicherheiten:

$$
log_2(16)+log_2(16)=8~bits
$$

Wenn wir stattdessen Wahrscheinlichkeiten verwenden, müssten wir die Chancen multiplizieren, um die Wahrscheinlichkeit zu finden, beide Zahlen korrekt zu erraten:

$$
P_{correct,correct}=\frac{1}{16}\times\frac{1}{16}=\frac{1}{256}
$$

Mit beiden Zahlen gibt es 256 Möglichkeiten. Mit Shannons Methode erhalten wir das gleiche Ergebnis:

$$
log_2(256)=8~bits
$$

Wie du siehst, vereinfacht die Verwendung von Logarithmen unsere Berechnungen, besonders wenn es um mehrere Quellen der Unsicherheit geht. Diese Klarheit und Einfachheit sind der Grund, warum Shannons Ansatz fundamental für die Informationstheorie geworden ist.

### Information

Um den Begriff *Information* aus dem Begriff der *Unsicherheit* herzuleiten, betrachten wir zwei Zustände: Die ursprüngliche Unsicherheit vor einer Frage, die wir als $H_0$ bezeichnen, und die reduzierte Unsicherheit nach der Antwort, die wir $H_1$ nennen. Die Menge an Information $I$, die wir durch die Antwort gewinnen, entspricht der Differenz dieser beiden Unsicherheiten:

$$
I = H_0 - H_1
$$

Information ist also die Reduzierung von Unsicherheit. Mit der obigen Formel können wir die Information $I$ präzise quantifizieren. Dies ermöglicht uns zu berechnen, wie viel Information wir durch jede Frage gewinnen. Betrachten wir ein konkretes Beispiel aus unserem Zahlenratespiel: Angenommen, deine erste Frage ist "Ist deine Zahl größer als 12?" und die Antwort lautet "nein". Dann bleiben die Zahlen 1 bis 12 als Möglichkeiten übrig. Die gewonnene Information berechnet sich wie folgt:

$$
I = log_2(16) - log_2(12) \approx 4 - 3.585 \approx 0.415~bits
$$

In diesem Fall lieferte die Antwort etwa 0,415 Bits an Information – also weniger als ein Bit. Das ist folgerichtig, da die Antwort weniger als die Hälfte der Möglichkeiten eliminiert hat.

Wie sähe es aus, wenn die Antwort "ja" gewesen wäre? In diesem Fall blieben nur die Zahlen 13, 14, 15 und 16 übrig, also vier mögliche Optionen:

$$
I = log_2(16) - log_2(4) = 4-2= 2~bits
$$

Diese Antwort lieferte mehr als ein Bit, nämlich 2 Bits. Wie lässt sich dieser Unterschied erklären?

### Wahrscheinliche und unwahrscheinliche Antworten

Die Menge an Information hängt von der Wahrscheinlichkeit der jeweiligen Antwort ab. Bei einer Frage, die die Möglichkeiten halbiert, hat jede Antwort "Ja" oder "Nein" auf die Frage "Ist die Zahl größer als X?" eine Wahrscheinlichkeit von genau 50%. Diese gleichmäßige Verteilung vereinfacht die Berechnung und liefert genau ein Bit an Information.

Bei der Frage "Ist deine Zahl größer als 12?" sind die beiden Antworten "Ja" und "Nein" allerdings nicht gleich wahrscheinlich. Es gibt zwölf mögliche Zahlen, die ein "Nein" ergeben und nur vier, die ein "Ja" ergeben, was Wahrscheinlichkeiten von 75% für "Nein" und 25% für "Ja" bedeutet. Da die "Ja"-Antwort weniger wahrscheinlich ist, ist sie überraschender – und liefert deshalb mehr Information. Wie wir berechnet haben, gibt uns ein "Ja" 2 Bits an Information, während ein "Nein" nur 0,415 Bits liefert.

Dieser Zusammenhang ist ein Kernprinzip der Informationstheorie: Je unwahrscheinlicher ein Ereignis ist, desto mehr Information enthält sein Auftreten. Umgekehrt liefert eine sehr wahrscheinliche Antwort, wie das "Nein" im obigen Beispiel, weniger Information, da sie weniger überraschend ist.

Warum zielen wir darauf ab, Fragen zu stellen, die die Möglichkeiten gleichmäßig halbieren? Man könnte auch wild raten in der Hoffnung, durch eine unwahrscheinlichere Antwort mehr Informationen zu bekommen – auch wenn man seltener richtig liegt. Die Antwort ist einfach: Eine Frage, die den Raum der Möglichkeiten halbiert, maximiert unseren *erwarteten* Informationsgewinn. Zwar können wir mit einer riskanten Frage wie "Ist die Zahl größer 15?" unser Glück herausfordern und im Erfolgsfall die Unsicherheit stark verringern. Dies ist jedoch keine nachhaltige Strategie. Wenn wir das Spiel nicht nur einmal, sondern 10, 100 oder 1000 Mal spielen, nähern wir uns im Mittel dem Erwartungswert für die gewonnene Information $E[I]$. Und dieser Erwartungswert favorisiert eindeutig Fragen, die die Hälfte der Möglichkeiten eliminieren. 

Um dies zu verdeutlichen, berechnen wir zunächst den Erwartungswert für die extreme Frage "Ist die Zahl größer 15?":

$$
E[I] = \left( \frac{1}{16}\times 4 \right)+ \left(\frac{15}{16}\times 0.09311 \right)= 0.3373
$$

Der Erwartungswert berechnet sich durch den mit der Wahrscheinlichkeit gewichteten Informationsgewinn für jede mögliche Antwort. Bei der Frage "Ist die Zahl größer als 15?" gibt es ein "Ja" nur dann, wenn die Zahl 16 ist. Von den sechzehn möglichen Zahlen erzeugt also nur eine diese Antwort. Die Wahrscheinlichkeit beträgt somit $\frac{1}{16}$. In diesem Fall reduziert sich unsere Unsicherheit sofort auf Null, da nur noch eine einzige Möglichkeit übrig bleibt. Die ursprüngliche Unsicherheit betrug 4 Bits:

$$
H_0 = log_2(16) = 4~bits
$$

Und wenn die Unsicherheit nach der Antwort "Ja" auf Null reduziert wird:

$$
H_1 = log_2(1) = 0~bits
$$

Daraus ergibt sich ein Informationsgehalt von 4 Bits:

$$
I = H_0 - H_1 = 4 - 0 = 4~bits
$$

Dies erklärt den ersten Teil der Erwartungswertgleichung. Der zweite Teil folgt demselben Prinzip, allerdings mit einem wichtigen Unterschied: Die Wahrscheinlichkeit für die Antwort "Nein" ist mit $\frac{15}{16}$ deutlich höher. Gleichzeitig verbleibt eine große Restunsicherheit $H_1$, da wir lediglich eine einzige Zahl ausschließen konnten:

$$
H_1 = log_2(15) \approx 3.9069~bits
$$

Die gewonnene Information ist somit sehr klein:

$$
I = 4 - 3.9069 \approx 0.0931~bits
$$

Anhand der Erwartungswertformel konnten wir zeigen, dass eine extreme Frage, die mit viel Glück direkt zum Ziel führt, in unserem Ratespiel einen erwarteten Informationsgehalt von etwa einem Drittel Bit (0,3373) hat. Prüfen wir nun, ob der Erwartungswert für Fragen, die die Hälfte der Möglichkeiten eliminieren, tatsächlich höher ist:

$$
E[I] = \left(0.5 \times 1 \right) + \left(0.5 \times 1 \right) = 0.5 + 0.5 = 1~bit
$$

Wie erwartet beträgt der Erwartungswert genau 1 Bit, da bei jeder Antwort – ob "Ja" oder "Nein" – jeweils die Hälfte der Optionen eliminiert wird.

### Mehr als zwei Antworten

Bisher haben wir in unserem Zahlenratespiel nur Fragen mit zwei Antwortmöglichkeiten betrachtet. Informationen, also Antworten auf Fragen, können jedoch vielfältiger sein. Wie berechnen wir die Information, wenn es mehr als zwei Antwortmöglichkeiten gibt?

Als Beispiel betrachten wir eine Urne mit farbigen Kugeln in drei Farben: Rot, Grün und Blau. Aus dieser Urne ziehen wir zwei Kugeln, wobei wir nach jedem Zug die gezogene Kugel zurück in die Urne legen. Dieses statistische Experiment bezeichnet man als "Ziehen mit Zurücklegen".

## Wovon hängt der Informationsgehalt einer Nachricht ab?

Nach den obigen Ausführungen können wir die Frage nun präzise beantworten: Der Informationsgehalt einer Nachricht hängt hauptsächlich von zwei Faktoren ab: **(1)** der *Anzahl der möglichen Alternativen* und **(2)** deren *Wahrscheinlichkeiten*.

### **Anzahl der Alternativen**

Je mehr verschiedene Möglichkeiten es gibt, was eine Nachricht aussagen kann, desto mehr Information kann im besten Fall vermittelt werden. Wenn zum Beispiel ein Lehrbuch-Test eine von zwei Antworten richtig haben kann (etwa wahr/falsch), dann liefert die Korrektur „richtig“ oder „falsch“ maximal 1 Bit Information – es gibt ja nur zwei mögliche Ausgänge. Hat eine Prüfungsfrage aber vier mögliche Antworten (A, B, C, D) von denen genau eine richtig ist, dann liefert die Angabe der korrekten Antwort bis zu $\log_2(4) = 2$ Bit Information. Allgemein: Ist eine Nachricht eine Auswahl aus $N$ gleich möglichen Alternativen, beträgt ihr Informationsgehalt $\log_2 N$ Bit. Mehr Alternativen bedeuten einen höheren potentiellen Informationsgehalt.

### **Wahrscheinlichkeiten der Alternativen**

Oft sind manche Nachrichten oder Ereignisse wahrscheinlicher als andere. Wie wir gesehen haben, tragen unwahrscheinliche (überraschende) Ereignisse mehr Information bei als sehr wahrscheinliche. Das klassische Beispiel ist das Wetter: In einer Region, in der es normalerweise jeden Tag sonnig ist, enthält die Nachricht „Heute scheint die Sonne“ kaum neue Information – es war ja ohnehin zu erwarten. Dagegen wäre in dieser Region die Nachricht „Heute regnet es“ deutlich informativer (weil Regen dort selten ist). In einer Regenwald-Region wäre es umgekehrt. **Der Informationsgehalt ist also höher, je unwahrscheinlicher (überraschender) der Inhalt der Nachricht ist.** Formal spiegelt das die Formel $I(p) = -\log_2 p$ wider: Kleine Wahrscheinlichkeit $p$ (seltenes Ereignis) ergibt großes $I(p)$.

### **Länge der Nachricht (Anzahl Zeichen)**

Ein weiterer offensichtlicher Faktor ist die Menge an Zeichen oder Symbolen, aus denen sich die Nachricht zusammensetzt. Eine längere Nachricht kann natürlich mehr Information tragen als eine sehr kurze, da sie mehr Inhalte übermitteln kann. Allerdings hängt das nicht nur von der Länge ab, sondern auch davon, ob die zusätzlichen Zeichen tatsächlich *neue* Information liefern oder nur bereits Bekanntes wiederholen. Mehr dazu gleich beim Thema Redundanz. Aber grundsätzlich können wir sagen: Der Informationsgehalt einer längeren Nachricht, die aus mehreren Symbolen besteht, ist – bei unabhängigen Symbolen – die Summe der Informationsgehalte der einzelnen Symbole

**Beispiel**: Besteht eine Nachricht aus 3 Buchstaben, und jeder Buchstabe trägt im Durchschnitt 4 Bit Information, dann hat die ganze Nachricht etwa $3 \cdot 4 = 12$ Bit Informationsgehalt (unter der Annahme, dass die Buchstaben unabhängig und nicht vorhersagbar voneinander sind).

Zusammengefasst: **Der Informationsgehalt einer Nachricht hängt von der Unsicherheit ab, die sie ausräumt.** Diese Unsicherheit wiederum ergibt sich aus der Zahl der möglichen Nachrichten und deren Verteilung. Maximal informativ ist eine Nachricht, die aus vielen möglichen, gleich wahrscheinlichen Alternativen ausgewählt wurde – dann ist die Überraschung am größten. Wenn einige Ergebnisse schon vorher sehr wahrscheinlich sind, verringert sich der Informationsgehalt entsprechend.

**Fachbegriffe**: Informationsgehalt bezeichnet die Menge an Information, die eine konkrete Nachricht liefert. Sie ist umso höher, je mehr mögliche Alternativen es gab und je unwahrscheinlicher die empfangene Nachricht war. Bei zusammengesetzten Nachrichten addieren sich die Informationsgehalte der Teile (wenn sie unabhängig voneinander sind).

## Was ist Redundanz und welche Rolle spielt sie?

## Welcher Zusammenhang besteht zwischen Information und Daten

Die kurze Antwort lautet: Daten repräsentieren Informationen. Daten sind die physische oder digitale Repräsentation von Informationen in einer bestimmten Form oder Struktur. Wenn wir zum Beispiel die Zahl "42" auf Papier schreiben oder in einem Computer speichern, dann sind die geschriebenen Ziffern oder die Bits im Speicher die Daten, während die tatsächliche Bedeutung – also die Zahl zweiundvierzig – die Information ist. Vielleicht eine Antwort auf eine Frage wie die nach dem Sinn des Lebens? Ein Computer verarbeitet dabei ausschließlich Daten in Form von Bits, die dann durch entsprechende Programme wieder in für Menschen verständliche Informationen umgewandelt werden. Dazu verwenden wir dann die Codesysteme, die wir in @sec-codesystems kennenlernen.

